{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":2984728,"sourceType":"datasetVersion","datasetId":1829286}],"dockerImageVersionId":30558,"isInternetEnabled":false,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Breast Cancer Classification using Machine Learning Algorithms","metadata":{}},{"cell_type":"markdown","source":"<p>\n    <b>Description:</b> Breast cancer is the most common cancer amongst women in the world. It accounts for 25% of all cancer cases, and affected over 2.1 Million people in 2015 alone. It starts when cells in the breast begin to grow out of control. These cells usually form tumors that can be seen via X-ray or felt as lumps in the breast area.\n</p>\n<p>\n   <b>Problem: </b>The key challenge is how to classify tumors into malignant (cancerous) or benign (non cancerous).\n</p>","metadata":{}},{"cell_type":"markdown","source":"### Importing Libraries and Dataset","metadata":{}},{"cell_type":"code","source":"#\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n#\nfrom sklearn.datasets import load_breast_cancer\n#\nfrom sklearn.model_selection import train_test_split\n#\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.metrics import accuracy_score, confusion_matrix, classification_report","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Loading the dataset\ndataset = load_breast_cancer()","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Creating a dataframe for the dataset\ndf = pd.DataFrame(dataset.data, columns = dataset['feature_names'])","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# A sample from the dataset\ndf.sample(10)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Adding the target column to the dataframe\ndf['target'] = dataset['target']","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df.sample(5)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Getting statistical description of the columns in the dataset\ndf.describe()","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Getting general information about the columns in the dataframe\ndf.info()","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Exploratory Data Analysis (EDA) and Data Pre-processing","metadata":{}},{"cell_type":"markdown","source":"#### NaN values","metadata":{}},{"cell_type":"code","source":"df.isna().sum()","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### Univariate, Bivariate, and Multivariate Analysis","metadata":{}},{"cell_type":"markdown","source":"#### Univariate Analysis","metadata":{}},{"cell_type":"code","source":"df.columns","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Column: mean radius\nfig, (ax1, ax2) = plt.subplots(1,2, figsize=(20,8))\nsns.histplot(df['mean radius'], ax=ax1, bins = 15)\nsns.boxplot(x = df['mean radius'], ax = ax2)\nplt.show()","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Column: mean radius\nfig, (ax1, ax2) = plt.subplots(1,2, figsize=(20,8))\nsns.histplot(df['mean texture'], ax=ax1, bins = 15)\nsns.boxplot(x = df['mean texture'], ax = ax2)\nplt.show()","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df.columns","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"## Plotting the histogram and boxplot plots for all the numerical columns\n\n# Getting the list of numerical/float columns except the target column 'target'\nlist_num_cols = list(df.columns[:-1])\n\n# A for loop to create histogram and boxplot plots for each of the columns in 'list_num_cols'\nfor col in list_num_cols:\n    print(\"Histogram plot and Box plot: \", col)\n    fig, (ax1, ax2) = plt.subplots(1,2, figsize=(20,8))\n    sns.histplot(df[col], ax=ax1, bins = 15)\n    sns.boxplot(x = df[col], ax = ax2)\n    plt.show()","metadata":{"scrolled":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### Bivariate Analysis Between The Features and The Target Columns","metadata":{}},{"cell_type":"code","source":"df.target.value_counts()","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<b>Note:</b> Class Distribution: 212 - Malignant, 357 - Benign","metadata":{}},{"cell_type":"code","source":"sns.barplot(x = df.target.value_counts().index, y = df.target.value_counts().values)\nplt.show()","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df.columns","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Column: mean radius\nsns.histplot(data = df, x = \"mean radius\", hue = \"target\")","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Column: mean texture\nsns.histplot(data = df, x = \"mean texture\", hue = \"target\")","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"## Plotting the overlapping histograms between the numerical features and the target\n\n# Getting the list of numerical/float columns except the target column 'target'\nlist_num_cols = list(df.columns[:-1])\n\n# A for loop to create the overlapping histograms\nfor col in list_num_cols:\n    print(\"Histogram plot: \", col)\n    sns.histplot(data = df, x = col, hue = \"target\")\n    plt.show()","metadata":{"scrolled":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### Features Engineering","metadata":{}},{"cell_type":"code","source":"# Checking the correlations between the columns (features and target)\n\ncorr = df.corr().abs()\n\n# Mask the upper triangle\nmask = np.triu(np.ones_like(corr, dtype=bool))\n\nplt.figure(figsize=(20,20))\nsns.heatmap(corr, annot = True, mask = mask)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Get only the correlations with the target column\nplt.figure(figsize=(5,10))\nsns.heatmap(df.corr().abs()[['target']].sort_values(by = 'target', ascending = False), annot = True)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df.corr().abs()['target'].sort_values()","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<b>Note:</b> You can consider only selecting the columns/features that are highly correlated with the output target, or you can use them all.\n<br>\n<b>Note 2:</b> But, usually, selecting highly correlated features with the target gives better results!","metadata":{}},{"cell_type":"markdown","source":"#### Models Developments [Logistic Regression and Decision Trees]","metadata":{}},{"cell_type":"code","source":"# Defining the features and the target\nfeatures = df.drop(columns = ['target'])\ntarget = df.target","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Splitting the data to training and test sets\nX_train, X_test, y_train, y_test = train_test_split(features, target, test_size = 0.3)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<b>Note:</b> You can add the hyperparameters tunining/optimization part, to find the best model's hyperparameters, but, I am not going to do it at the moment!","metadata":{}},{"cell_type":"code","source":"## Logistic Regerssion\n# Initializing the LogisticRegression model\nLR_model = LogisticRegression(max_iter = 3000)\n# Training the model, by fitting the data to it\nLR_model.fit(X_train, y_train)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Getting the LogisticRegression predictions on the test set (unseen data)\nLR_predictions = LR_model.predict(X_test)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Printing the classification report (actual values vs. predictions)\nprint(classification_report(y_test, LR_predictions))","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Showing the confusion matrix\nsns.heatmap(confusion_matrix(y_test, LR_predictions), annot = True)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<p>\n    <b>Remember:</b> Class Distribution: 212 - Malignant (Class 0 - Negative class), 357 - Benign (Class 1 - Positive class) \n</p>\n<p>\n    <b>Note:</b> Do not forget that we are dealing with a problem, in which minimizing the False Positive predictions (actual negative (malignant) and predicted positive (benign) ) is more important than the False Negative predictions (actual positive (benign) and predicted negative (malignant) ), that is wht it is important to maximize the precision metric\n</p>","metadata":{}},{"cell_type":"code","source":"## Decision Trees\n# Initializing the Decision Tree model\nDT_model = DecisionTreeClassifier()\n# Training the model, by fitting the data to it\nDT_model.fit(X_train, y_train)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Getting the Decision Tree predictions on the test set (unseen data)\nDT_predictions = DT_model.predict(X_test)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Printing the classification report (actual values vs. predictions)\nprint(classification_report(y_test, DT_predictions))","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Showing the confusion matrix\nsns.heatmap(confusion_matrix(y_test, DT_predictions), annot = True)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<p>\n    <b>Remember:</b> Class Distribution: 212 - Malignant (Class 0 - Negative class), 357 - Benign (Class 1 - Positive class) \n</p>\n<p>\n    <b>Note:</b> Do not forget that we are dealing with a problem, in which minimizing the False Positive predictions (actual negative (malignant) and predicted positive (benign) ) is more important than the False Negative predictions (actual positive (benign) and predicted negative (malignant) ), that is wht it is important to maximize the precision metric\n</p>","metadata":{}}]}